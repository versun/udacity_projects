{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迷你项目：动态规划\n",
    "\n",
    "在此 notebook 中，你将自己编写很多经典动态规划算法的实现。\n",
    "\n",
    "虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。\n",
    "\n",
    "### 第 0 部分：探索 FrozenLakeEnv\n",
    "\n",
    "请使用以下代码单元格创建 [FrozenLake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) 环境的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozenlake import FrozenLakeEnv\n",
    "\n",
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体将会在 $4 \\times 4$ 网格世界中移动，状态编号如下所示："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体可以执行 4 个潜在动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，$\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$ 以及 $\\mathcal{A} = \\{0, 1, 2, 3\\}$。请通过运行以下代码单元格验证这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动态规划假设智能体完全了解 MDP。我们已经修改了 `frozenlake.py` 文件以使智能体能够访问一步动态特性。  \n",
    "\n",
    "请执行以下代码单元格以返回特定状态和动作对应的一步动态特性。具体而言，当智能体在网格世界中以状态 1 向左移动时，`env.P[1][0]` 会返回每个潜在奖励的概率和下一个状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 1L, 0.0, False),\n",
       " (0.3333333333333333, 0L, 0.0, False),\n",
       " (0.3333333333333333, 5L, 0.0, True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个条目的格式如下所示"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prob, next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "- `prob` 详细说明了相应的  (`next_state`, `reward`) 对的条件概率，以及\n",
    "- 如果 `next_state` 是终止状态，则 `done` 是 `True` ，否则是 `False`。\n",
    "\n",
    "因此，我们可以按照以下方式解析 `env.P[1][0]`：\n",
    "$$\n",
    "\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases}\n",
    "               \\frac{1}{3} \\text{ if } s'=1, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=0, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=5, r=0\\\\\n",
    "               0 \\text{ else}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "你可以随意更改上述代码单元格，以探索在其他（状态、动作）对下环境的行为是怎样的。\n",
    "\n",
    "### 第 1 部分：迭代策略评估\n",
    "\n",
    "在此部分，你将自己编写迭代策略评估的实现。\n",
    "\n",
    "你的算法应该有四个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "- `theta`：这是一个非常小的正数，用于判断估算值是否足够地收敛于真值函数 (默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `V`：这是一个一维numpy数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 在输入策略下的估算值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "            V[s] = Vs\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将评估等概率随机策略  $\\pi$，其中对于所有 $s\\in\\mathcal{S}$ 和 $a\\in\\mathcal{A}(s)$ ，$\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$。  \n",
    "\n",
    "请使用以下代码单元格在变量 `random_policy`中指定该策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以评估等概率随机策略并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plot_utils import plot_values\n",
    "\n",
    "# evaluate the policy \n",
    "V = policy_evaluation(env, random_policy)\n",
    "\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_13_0.png)\n",
    "\n",
    "\n",
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！  \n",
    "\n",
    "**注意：**为了确保结果准确，确保你的 `policy_evaluation` 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no such test method in <class 'check_test.Tests'>: runTest",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-24f1e5f34967>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrun_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'policy_evaluation_check'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\git\\udacity_projects\\2-机器学习工程师\\E-强化学习\\1-迷你项目：动态规划 (OpenAI Gym-FrozenLakeEnv)\\check_test.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_iteration_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\unittest\\case.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, methodName)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             raise ValueError(\"no such test method in %s: %s\" %\n\u001b[1;32m--> 189\u001b[1;33m                   (self.__class__, methodName))\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_testMethodDoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestMethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cleanups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: no such test method in <class 'check_test.Tests'>: runTest"
     ]
    }
   ],
   "source": [
    "import check_test\n",
    "\n",
    "run_check('policy_evaluation_check', policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**\n",
    "\n",
    "\n",
    "### 第 2 部分：通过 $v_\\pi$ 获取 $q_\\pi$\n",
    "\n",
    "在此部分，你将编写一个函数，该函数的输入是状态值函数估值以及一些状态 $s\\in\\mathcal{S}$。它会返回输入状态 $s\\in\\mathcal{S}$ 对应的**动作值函数中的行**。即你的函数应同时接受输入 $v_\\pi$ 和 $s$，并针对所有 $a\\in\\mathcal{A}(s)$ 返回 $q_\\pi(s,a)$。\n",
    "\n",
    "你的算法应该有四个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "- `s`：这是环境中的状态对应的整数。它应该是在 `0` 到 `(env.nS)-1`（含）之间的值。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `q`：这是一个一维 numpy 数组，其中 `q.shape[0]` 等于动作数量 (`env.nA`)。`q[a]` 包含状态 `s` 和动作 `a` 的（估算）值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_from_v(env, V, s, gamma=1):\n",
    "    q = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q[a] += prob * (reward + gamma * V[next_state])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请运行以下代码单元格以输出上述状态值函数对应的动作值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-Value Function:\n",
      "[[0.0147094  0.01393978 0.01393978 0.01317015]\n",
      " [0.00852356 0.01163091 0.0108613  0.01550788]\n",
      " [0.02444514 0.02095298 0.02406033 0.01435346]\n",
      " [0.01047649 0.01047649 0.00698432 0.01396865]\n",
      " [0.02166487 0.01701828 0.01624865 0.01006281]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05433538 0.04735105 0.05433538 0.00698432]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01701828 0.04099204 0.03480619 0.04640826]\n",
      " [0.07020885 0.11755991 0.10595784 0.05895312]\n",
      " [0.18940421 0.17582037 0.16001424 0.04297382]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.08799677 0.20503718 0.23442716 0.17582037]\n",
      " [0.25238823 0.53837051 0.52711478 0.43929118]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros([env.nS, env.nA])\n",
    "for s in range(env.nS):\n",
    "    Q[s] = q_from_v(env, V, s)\n",
    "print(\"Action-Value Function:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-41da055744f7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-41da055744f7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Action-Value Function:\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Action-Value Function:\n",
    "[[ 0.0147094   0.01393978  0.01393978  0.01317015]\n",
    " [ 0.00852356  0.01163091  0.0108613   0.01550788]\n",
    " [ 0.02444514  0.02095298  0.02406033  0.01435346]\n",
    " [ 0.01047649  0.01047649  0.00698432  0.01396865]\n",
    " [ 0.02166487  0.01701828  0.01624865  0.01006281]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05433538  0.04735105  0.05433538  0.00698432]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.01701828  0.04099204  0.03480619  0.04640826]\n",
    " [ 0.07020885  0.11755991  0.10595784  0.05895312]\n",
    " [ 0.18940421  0.17582037  0.16001424  0.04297382]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.08799677  0.20503718  0.23442716  0.17582037]\n",
    " [ 0.25238823  0.53837051  0.52711478  0.43929118]\n",
    " [ 0.          0.          0.          0.        ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！  \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `q_from_v` 函数满足上文列出的要求（具有四个输入、一个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e41fa2c0027c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheck_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q_from_v_check'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_from_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'check_test' is not defined"
     ]
    }
   ],
   "source": [
    "check_test.run_check('q_from_v_check', q_from_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**\n",
    "\n",
    "\n",
    "### 第 3 部分：策略改进\n",
    "\n",
    "在此部分，你将自己编写策略改进实现。 \n",
    "\n",
    "你的算法应该有三个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "\n",
    "请完成以下代码单元格中的函数。建议你使用你在上文实现的 `q_from_v` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    for s in range(env.nS):\n",
    "        q = q_from_v(env, V, s, gamma)\n",
    "        \n",
    "        # OPTION 1: construct a deterministic policy \n",
    "        # policy[s][np.argmax(q)] = 1\n",
    "        \n",
    "        # OPTION 2: construct a stochastic policy that puts equal probability on maximizing actions\n",
    "        best_a = np.argwhere(q==np.max(q)).flatten()\n",
    "        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！  \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `policy_improvement` 函数满足上文列出的要求（具有三个输入、一个输出，并且没有更改输入参数的默认值）。\n",
    "\n",
    "在继续转到该 notebook 的下个部分之前，强烈建议你参阅 **Dynamic_Programming_Solution.ipynb** 中的解决方案。该函数有很多正确的实现方式！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('policy_improvement_check', policy_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**\n",
    "\n",
    "\n",
    "### 第 4 部分：策略迭代\n",
    "\n",
    "在此部分，你将自己编写策略迭代的实现。该算法会返回最优策略，以及相应的状态值函数。\n",
    "\n",
    "你的算法应该有三个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "- `theta`：这是一个非常小的正数，用于判断策略评估步骤是否足够地收敛于真值函数 (默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "\n",
    "请完成以下代码单元格中的函数。强烈建议你使用你在上文实现的 `policy_evaluation` 和 `policy_improvement` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def policy_iteration(env, gamma=1, theta=1e-8):\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        V = policy_evaluation(env, policy, gamma, theta)\n",
    "        new_policy = policy_improvement(env, V)\n",
    "        \n",
    "        # OPTION 1: stop if the policy is unchanged after an improvement step\n",
    "        if (new_policy == policy).all():\n",
    "            break;\n",
    "        \n",
    "        # OPTION 2: stop if the value function estimates for successive policies has converged\n",
    "        # if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:\n",
    "        #    break;\n",
    "        \n",
    "        policy = copy.copy(new_policy)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以解决该 MDP 并可视化输出结果。最优状态值函数已调整形状，以匹配网格世界的形状。\n",
    "\n",
    "**将该最优状态值函数与此 notebook 第 1 部分的状态值函数进行比较**。_最优状态值函数一直都大于或等于等概率随机策略的状态值函数吗？_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the optimal policy and optimal state-value function\n",
    "policy_pi, V_pi = policy_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_pi,\"\\n\")\n",
    "\n",
    "plot_values(V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\n",
    "[[ 1.    0.    0.    0.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 1.    0.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.5   0.    0.5   0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    1.    0.    0.  ]\n",
    " [ 1.    0.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.    0.    1.    0.  ]\n",
    " [ 0.    1.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_29_1.png)\n",
    "\n",
    "\n",
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！  \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `policy_iteratio` 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('policy_iteration_check', policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**\n",
    "\n",
    "\n",
    "### 第 5 部分：截断策略迭代\n",
    "\n",
    "在此部分，你将自己编写截断策略迭代的实现。  \n",
    "\n",
    "首先，你将实现截断策略评估。你的算法应该有五个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "- `max_it`：这是一个正整数，对应的是经历状态空间的次数（默认值为：`1`）。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_evaluation(env, policy, V, max_it=1, gamma=1):\n",
    "    num_it=0\n",
    "    while num_it < max_it:\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            q = q_from_v(env, V, s, gamma)\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                v += action_prob * q[a]\n",
    "            V[s] = v\n",
    "        num_it += 1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，你将实现截断策略迭代。你的算法应该接受五个**输入**参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `max_it`：这是一个正整数，对应的是经历状态空间的次数（默认值为：`1`）。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "- `theta`：这是一个非常小的正整数，用作停止条件（默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_iteration(env, max_it=1, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        policy = policy_improvement(env, V)\n",
    "        old_V = copy.copy(V)\n",
    "        V = truncated_policy_evaluation(env, policy, V, max_it, gamma)\n",
    "        if max(abs(V-old_V)) < theta:\n",
    "            break;\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。\n",
    "\n",
    "请实验不同的 `max_it` 参数值。始终都能获得最优状态值函数吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=2)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_tpi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_tpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\n",
    "[[ 1.    0.    0.    0.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 1.    0.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.5   0.    0.5   0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    1.    0.    0.  ]\n",
    " [ 1.    0.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.    0.    1.    0.  ]\n",
    " [ 0.    1.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_37_1.png)\n",
    "\n",
    "\n",
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `truncated_policy_iteration` 函数满足上文列出的要求（具有四个输入、两个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('truncated_policy_iteration_check', truncated_policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**\n",
    "\n",
    "\n",
    "### 第 6 部分：值迭代\n",
    "\n",
    "在此部分，你将自己编写值迭代的实现。\n",
    "\n",
    "你的算法应该接受三个输入参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例，其中 `env.P` 会返回一步动态特性。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。 \n",
    "- `theta`：这是一个非常小的正整数，用作停止条件（默认值为：`1e-8`）。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `policy`：这是一个二维 numpy 数组，其中 `policy.shape[0]` 等于状态数量 (`env.nS`) ， `policy.shape[1]` 等于动作数量 (`env.nA`) 。`policy[s][a]`  返回智能体在状态 `s` 时根据该策略选择动作 `a` 的概率。\n",
    "- `V`：这是一个一维 numpy 数组，其中 `V.shape[0]` 等于状态数量 (`env.nS`)。`V[s]` 包含状态 `s` 的估值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max(q_from_v(env, V, s, gamma))\n",
    "            delta = max(delta,abs(V[s]-v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下个代码单元格以解决该 MDP 并可视化输出结果。状态值函数已调整形状，以匹配网格世界的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_vi, V_vi = value_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\n",
    "[[ 1.    0.    0.    0.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 1.    0.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.5   0.    0.5   0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.    0.    0.    1.  ]\n",
    " [ 0.    1.    0.    0.  ]\n",
    " [ 1.    0.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.25  0.25  0.25  0.25]\n",
    " [ 0.    0.    1.    0.  ]\n",
    " [ 0.    1.    0.    0.  ]\n",
    " [ 0.25  0.25  0.25  0.25]] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_43_1.png)\n",
    "\n",
    "\n",
    "运行以下代码单元格以测试你的函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！ \n",
    "\n",
    "**注意：**为了确保结果准确，确保 `truncated_policy_iteration` 函数满足上文列出的要求（具有三个输入、两个输出，并且没有更改输入参数的默认值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('value_iteration_check', value_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
